{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChorinVortex_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbEgqCfKoNUm"
      },
      "source": [
        "# SETUP\n",
        "import random\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import itertools\n",
        "import math\n",
        "import time\n",
        "import tensorflow_probability as tfp\n",
        "import functools\n",
        "\n",
        "pi = math.pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipQcEGgiaDap"
      },
      "source": [
        "# GENERATING DATA (AND WRITE TO .CSV)\n",
        "def flatArr(x):\n",
        "    y = []\n",
        "    for row in x:\n",
        "        for el in row:\n",
        "            y.append(el)\n",
        "    return np.array(y).flatten()\n",
        "\n",
        "def sampling(x_range, y_range, t_range, t_size, int_size, bound_size):\n",
        "    int_x = np.random.uniform(x_range[0], x_range[1], int_size)\n",
        "    int_y = np.random.uniform(y_range[0], y_range[1], int_size)\n",
        "    Boundary_points = []\n",
        "    for i in range(bound_size):\n",
        "        random_point = [random.uniform(x_range[0], x_range[1]), random.uniform(y_range[0], y_range[1])]\n",
        "        random_index = random.randint(0,1)\n",
        "        if random_index == 0:\n",
        "            random_value = random.choice([x_range[0], x_range[1]])\n",
        "        else:\n",
        "            random_value = random.choice([y_range[0], y_range[1]])\n",
        "        random_point[random_index] = random_value\n",
        "        Boundary_points.append(random_point)\n",
        "    bound_x = np.array(Boundary_points)[:, 0]\n",
        "    bound_y = np.array(Boundary_points)[:, 1]\n",
        "    delta_t = t_range[1]/t_size\n",
        "    int_x_t = np.array([])\n",
        "    int_y_t = np.array([])\n",
        "    bound_x_t = np.array([])\n",
        "    bound_y_t = np.array([])\n",
        "    t = []\n",
        "    tb = []\n",
        "    for i in range(t_size):\n",
        "        t.append([round(t_range[0]+(i+1)*delta_t,6) for e in range(int_x.size)])\n",
        "        int_x_t = np.concatenate((int_x_t, int_x))\n",
        "        int_y_t = np.concatenate((int_y_t, int_y))\n",
        "        tb.append([round(t_range[0]+(i+1)*delta_t,6) for e in range(bound_x.size)])\n",
        "        bound_x_t = np.concatenate((bound_x_t, bound_x))\n",
        "        bound_y_t = np.concatenate((bound_y_t, bound_y))\n",
        "    t = flatArr(t)\n",
        "    tb = flatArr(tb)\n",
        "    # initial points----------------------------------------------\n",
        "    int_x = np.random.uniform(x_range[0], x_range[1], int(int_x_t.size/2))\n",
        "    int_y = np.random.uniform(y_range[0], y_range[1], int(int_x_t.size/2))\n",
        "    Boundary_points = []\n",
        "    for i in range(int(int_x_t.size/2)):\n",
        "        random_point = [random.uniform(x_range[0], x_range[1]), random.uniform(y_range[0], y_range[1])]\n",
        "        random_index = random.randint(0,1)\n",
        "        if random_index == 0:\n",
        "            random_value = random.choice([x_range[0], x_range[1]])\n",
        "        else:\n",
        "            random_value = random.choice([y_range[0], y_range[1]])\n",
        "        random_point[random_index] = random_value\n",
        "        Boundary_points.append(random_point)\n",
        "    bound_x = np.array(Boundary_points)[:, 0]\n",
        "    bound_y = np.array(Boundary_points)[:, 1]\n",
        "    int_x = np.concatenate((int_x, bound_x))\n",
        "    int_y = np.concatenate((int_y, bound_y))\n",
        "    return int_x, int_y, t, int_x_t, int_y_t, tb, bound_x_t, bound_y_t\n",
        "\n",
        "# training data\n",
        "N_train = 1000;        # number of points\n",
        "int_x, int_y, t, int_x_t, int_y_t, tb, bound_x_t, bound_y_t = sampling([0.0, 1.0], [0.0, 1.0], [0.0, 1.0], 10, N_train, N_train)\n",
        "\n",
        "# write training data to .csv file\n",
        "# with open('/content/drive/MyDrive/Papers/Code (chung)/data_train.csv', mode='w') as f:\n",
        "with open('data_train.csv', mode='w') as f:\n",
        "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    for i in range(t.size):\n",
        "        csv_writer.writerow([int_x_t[i], int_y_t[i], t[i], bound_x_t[i], bound_y_t[i], tb[i], int_x[i], int_y[i], 0.0])\n",
        "\n",
        "\n",
        "# test data\n",
        "N_test = 200;        # number of points\n",
        "int_x, int_y, t, int_x_t, int_y_t, tb, bound_x_t, bound_y_t = sampling([0.0, 1.0], [0.0, 1.0], [0.0, 1.0], 5, N_test, N_test)\n",
        "\n",
        "# write test data to .csv file\n",
        "# with open('/content/drive/MyDrive/Papers/Code (chung)/data_test.csv', mode='w') as f:\n",
        "with open('data_test.csv', mode='w') as f:\n",
        "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    for i in range(t.size):\n",
        "        csv_writer.writerow([int_x[i], int_y[i], 0.0])\n",
        "    for i in range(t.size):\n",
        "        csv_writer.writerow([int_x_t[i], int_y_t[i], t[i]])\n",
        "    for i in range(t.size):\n",
        "        csv_writer.writerow([bound_x_t[i], bound_y_t[i], tb[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-sE7XErpOrr"
      },
      "source": [
        "# READ DATA\n",
        "\n",
        "# load the dataset\n",
        "# data_train = np.float32(np.loadtxt('/content/drive/MyDrive/Papers/Code (chung)/data_train.csv', delimiter=','))\n",
        "# data_test = np.float32(np.loadtxt('/content/drive/MyDrive/Papers/Code (chung)/data_test.csv', delimiter=','))\n",
        "\n",
        "data_train = np.float32(np.loadtxt('data_train.csv', delimiter=','))\n",
        "data_test = np.float32(np.loadtxt('data_test.csv', delimiter=','))\n",
        "# data_train_test = np.float32(np.loadtxt('data_train_test.csv', delimiter=','))\n",
        "# # split into datasets\n",
        "# P_in = dataset[:,0:2]\n",
        "# P_b = dataset[:,2:4]\n",
        "\n",
        "# # Load NumPy arrays with tf.data.Dataset\n",
        "# P_in = tf.data.Dataset.from_tensor_slices(P_in)\n",
        "# P_b = tf.data.Dataset.from_tensor_slices(P_b)\n",
        "\n",
        "# # Shuffle and batch the datasets\n",
        "# BATCH_SIZE=32\n",
        "# SHUFFLE_BUFFER_SIZE=100\n",
        "\n",
        "# P_in = P_in.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "# P_b = P_b.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WjxaAVzbqnx",
        "outputId": "2ef0032a-8b33-4f8b-d66f-e063c97f905c"
      },
      "source": [
        "# BUILD NETWORK\n",
        "\n",
        "# dimension of input and output\n",
        "in_dim = 3\n",
        "out_dim = 3\n",
        "\n",
        "# number of neurons on each layer\n",
        "nn = [32, 32, 32, 32, 32]\n",
        "\n",
        "# input layer\n",
        "inputs = keras.Input(shape=(in_dim,), name='points')\n",
        "\n",
        "# hidden layers\n",
        "hidden = keras.layers.Dense(nn[0], activation='tanh', name='hidden_1')(inputs)\n",
        "for i in range(len(nn)-1):\n",
        "    hidden = keras.layers.Dense(nn[i+1], activation='tanh', name='hidden_' + str(i+2))(hidden)\n",
        "\n",
        "# output layer\n",
        "outputs = keras.layers.Dense(out_dim, activation='linear', name=\"u\")(hidden)\n",
        "\n",
        "# create network\n",
        "PDEmodel = keras.Model(inputs=inputs, outputs=outputs, name='chorinvorte')\n",
        "\n",
        "# show network details\n",
        "PDEmodel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"chorinvorte\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "points (InputLayer)          [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "hidden_1 (Dense)             (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "hidden_2 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "hidden_3 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "hidden_4 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "hidden_5 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "u (Dense)                    (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 4,451\n",
            "Trainable params: 4,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3V4S9yGbsMT"
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "# GRAD U\n",
        "def grad_u(X):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch(X)\n",
        "        uvp = PDEmodel(X)\n",
        "        u = uvp[:, 0]\n",
        "        v = uvp[:, 1]\n",
        "        p = uvp[:, 2]\n",
        "\n",
        "    u_t = tape.gradient(u, X)[:, 2]\n",
        "    u_x = tape.gradient(u, X)[:, 0]\n",
        "    u_y = tape.gradient(u, X)[:, 1]\n",
        "    v_t = tape.gradient(v, X)[:, 2]\n",
        "    v_x = tape.gradient(v, X)[:, 0]\n",
        "    v_y = tape.gradient(v, X)[:, 1]\n",
        "    p_x = tape.gradient(p, X)[:, 0]\n",
        "    p_y = tape.gradient(p, X)[:, 1]\n",
        "\n",
        "    return u, v, u_t, u_x, u_y, v_t, v_x, v_y, p_x, p_y\n",
        "\n",
        "# return 1D weights\n",
        "def get_weights():\n",
        "    w = []\n",
        "    for layer in PDEmodel.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    return w\n",
        "\n",
        "# get size of weights in each model's layer\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "for layer in PDEmodel.layers[1:]:\n",
        "    weights_biases = layer.get_weights()\n",
        "    sizes_w.append(weights_biases[0].flatten().size)\n",
        "    sizes_b.append(weights_biases[1].size)\n",
        "\n",
        "# convert 1D weights to multi dimension weights in each model's layer\n",
        "def set_weights(w):\n",
        "    for i, layer in enumerate(PDEmodel.layers[1:]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i+1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "\n",
        "losses_hist = []\n",
        "def train(epochs, BATCH_SIZE=128, SHUFFLE_BUFFER_SIZE=100, optimizer = 'adam'):\n",
        "    Re = 100.0\n",
        "    # split into datasets\n",
        "    P_in = data_train[:,0:3]\n",
        "    # P_in_i = P_in[0:N_train]\n",
        "    # P_in = P_in[N_train:]\n",
        "    P_b = data_train[:,3:6]\n",
        "    # P_b_i = P_b[0:N_train]\n",
        "    # P_b = P_b[N_train:]\n",
        "    # P_i = np.concatenate((P_in_i, P_b_i))\n",
        "    P_i = data_train[:, 6:9]\n",
        "    # P_i_tf = tf.convert_to_tensor(P_i, dtype=tf.float32)\n",
        "    # CHOOSE OPTIMIZER\n",
        "    if (optimizer == 'adam'):\n",
        "        optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999)\n",
        "        # Load NumPy arrays with tf.data.Dataset\n",
        "        P_in = tf.data.Dataset.from_tensor_slices(P_in)\n",
        "        P_b = tf.data.Dataset.from_tensor_slices(P_b)\n",
        "        P_i = tf.data.Dataset.from_tensor_slices(P_i)\n",
        "        # Shuffle and batch the datasets\n",
        "        P_in = P_in.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "        P_b = P_b.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "        P_i = P_i.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "        # for loop epochs\n",
        "        for epoch in range(epochs):\n",
        "            start_epoch = time.time()\n",
        "            # for loop iteration (all batches)\n",
        "            loss = [0]\n",
        "            for (P_in_batch, P_b_batch, P_i_batch) in itertools.zip_longest(P_in, P_b, P_i):\n",
        "                # interior loss\n",
        "                with tf.GradientTape() as tape:\n",
        "                    with tf.GradientTape(persistent=True) as tape1:\n",
        "                        tape1.watch(P_in_batch)\n",
        "                        u, v, u_t, u_x, u_y, v_t, v_x, v_y, p_x, p_y = grad_u(P_in_batch)\n",
        "\n",
        "                    # u_xx and u_yy\n",
        "                    u_xx = tape1.gradient(u_x, P_in_batch)[:,0]\n",
        "                    u_yy = tape1.gradient(u_y, P_in_batch)[:,1]\n",
        "                    v_xx = tape1.gradient(v_x, P_in_batch)[:,0]\n",
        "                    v_yy = tape1.gradient(v_y, P_in_batch)[:,1]\n",
        "                    \n",
        "                    # loss_in_batch\n",
        "                    f_u = u_t - 1/Re*(u_xx + u_yy) + u*u_x + v*u_y + p_x\n",
        "                    f_v = v_t - 1/Re*(v_xx + v_yy) + u*v_x + v*v_y + p_y\n",
        "                    div_u = u_x + v_y\n",
        "                    loss_in_batch = tf.reduce_mean(tf.math.square(f_u)) + \\\n",
        "                                    tf.reduce_mean(tf.math.square(f_v)) + \\\n",
        "                                    tf.reduce_mean(tf.math.square(div_u))\n",
        "                    \n",
        "                    # loss_b_batch\n",
        "                    uvp = PDEmodel(P_b_batch)\n",
        "                    u = uvp[:,0]\n",
        "                    v = uvp[:,1]\n",
        "                    loss_b_batch = tf.reduce_mean(tf.math.square(u + tf.math.cos(pi*P_b_batch[:,0])*tf.math.sin(pi*P_b_batch[:,1])*tf.math.exp(-2.0*pi*pi*P_b_batch[:,2]/Re))) + \\\n",
        "                                   tf.reduce_mean(tf.math.square(v - tf.math.sin(pi*P_b_batch[:,0])*tf.math.cos(pi*P_b_batch[:,1])*tf.math.exp(-2.0*pi*pi*P_b_batch[:,2]/Re)))\n",
        "                    \n",
        "                    # loss_i_batch\n",
        "                    # if (P_i_batch == None):\n",
        "                    #     P_i_batch = P_i_batch_None\n",
        "                    #     P_i_batch = P_i_tf\n",
        "                    uvp = PDEmodel(P_i_batch)\n",
        "                    u = uvp[:,0]\n",
        "                    v = uvp[:,1]\n",
        "                    loss_i_batch = tf.reduce_mean(tf.math.square(u+tf.math.cos(pi*P_i_batch[:,0])*tf.math.sin(pi*P_i_batch[:,1]))) + \\\n",
        "                                   tf.reduce_mean(tf.math.square(v-tf.math.sin(pi*P_i_batch[:,0])*tf.math.cos(pi*P_i_batch[:,1])))\n",
        "                    # P_i_batch_None = P_i_batch\n",
        "\n",
        "                    # total loss\n",
        "                    loss_batch = loss_in_batch + loss_b_batch + loss_i_batch\n",
        "                \n",
        "                # update paremeters\n",
        "                grads = tape.gradient(loss_batch, PDEmodel.weights)\n",
        "                optimizer.apply_gradients(zip(grads, PDEmodel.weights))\n",
        "                \n",
        "                # add loss_batch to loss\n",
        "                loss += loss_batch\n",
        "            \n",
        "            loss = loss/len(list(P_in))\n",
        "            print(\"[%4s] loss = %12.5f \\t %4.3fs\" % (epoch, loss, time.time() - start_epoch))\n",
        "            losses_hist.append(loss.numpy()[0])\n",
        "\n",
        "            # break condition\n",
        "            # if (epoch > 200):\n",
        "            #     if (losses_hist[epoch][0]/losses_hist[epoch-100][0] > 0.9):\n",
        "            #         break\n",
        "\n",
        "    if (optimizer == 'l-bfgs'):\n",
        "        P_in = tf.convert_to_tensor(P_in, dtype=tf.float32)\n",
        "        P_b = tf.convert_to_tensor(P_b, dtype=tf.float32)\n",
        "        P_i = tf.convert_to_tensor(P_i, dtype=tf.float32)\n",
        "        def function_factory(P_in, P_b, P_i):\n",
        "\n",
        "            def loss_grad(weights):\n",
        "                start_epoch = time.time()\n",
        "                with tf.GradientTape() as tape:\n",
        "                    with tf.GradientTape(persistent=True) as tape1:\n",
        "                        set_weights(weights)\n",
        "                        tape1.watch(P_in)\n",
        "                        u, v, u_t, u_x, u_y, v_t, v_x, v_y, p_x, p_y = grad_u(P_in)\n",
        "\n",
        "                    # u_xx and u_yy\n",
        "                    u_xx = tape1.gradient(u_x, P_in)[:,0]\n",
        "                    u_yy = tape1.gradient(u_y, P_in)[:,1]\n",
        "                    v_xx = tape1.gradient(v_x, P_in)[:,0]\n",
        "                    v_yy = tape1.gradient(v_y, P_in)[:,1]\n",
        "\n",
        "                    # loss_in\n",
        "                    # - pi/2*tf.math.sin(2*pi*P_in[:,1])*tf.math.exp(-4*pi*pi*P_in[:,0]/Re)\n",
        "                    # - pi/2*tf.math.sin(2*pi*P_in[:,2])*tf.math.exp(-4*pi*pi*P_in[:,0]/Re)\n",
        "                    f_u = u_t - (1.0/Re)*(u_xx + u_yy) + u*u_x + v*u_y + p_x \n",
        "                    f_v = v_t - (1.0/Re)*(v_xx + v_yy) + u*v_x + v*v_y + p_y \n",
        "                    div_u = u_x + v_y\n",
        "                    loss_in = tf.reduce_mean(tf.math.square(f_u)) + \\\n",
        "                              tf.reduce_mean(tf.math.square(f_v)) + \\\n",
        "                              tf.reduce_mean(tf.math.square(div_u))\n",
        "\n",
        "                    # loss_b\n",
        "                    uvp = PDEmodel(P_b)\n",
        "                    u = uvp[:,0]\n",
        "                    v = uvp[:,1]\n",
        "                    loss_b = tf.reduce_mean(tf.math.square(u + tf.math.cos(pi*P_b[:,0])*tf.math.sin(pi*P_b[:,1])*tf.math.exp(-2.0*pi*pi*P_b[:,2]/Re))) + \\\n",
        "                             tf.reduce_mean(tf.math.square(v - tf.math.sin(pi*P_b[:,0])*tf.math.cos(pi*P_b[:,1])*tf.math.exp(-2.0*pi*pi*P_b[:,2]/Re)))\n",
        "\n",
        "                    # loss_i\n",
        "                    uvp = PDEmodel(P_i)\n",
        "                    u = uvp[:,0]\n",
        "                    v = uvp[:,1]\n",
        "                    loss_i = tf.reduce_mean(tf.math.square(u+tf.math.cos(pi*P_i[:,0])*tf.math.sin(pi*P_i[:,1]))) + \\\n",
        "                             tf.reduce_mean(tf.math.square(v-tf.math.sin(pi*P_i[:,0])*tf.math.cos(pi*P_i[:,1])))\n",
        "\n",
        "                    # total loss\n",
        "                    loss = loss_in + loss_b + loss_i\n",
        "                    print(\"loss = %12.5f \\t %4.3fs\" % (loss, time.time() - start_epoch))\n",
        "                    losses_hist.append(loss.numpy())\n",
        "                grad = tape.gradient(loss, PDEmodel.weights)\n",
        "                grad_1D = []\n",
        "                for g in grad:\n",
        "                    grad_1D.append(tf.reshape(g, [-1]))\n",
        "                grad_1D = tf.concat(grad_1D, 0)\n",
        "                return loss, grad_1D\n",
        "            return loss_grad\n",
        "        # update paremeters\n",
        "\n",
        "        func = function_factory(P_in, P_b, P_i)\n",
        "\n",
        "        # @tf.function\n",
        "        # def epoch():\n",
        "        #     if opt_result is None:\n",
        "        #         return 0\n",
        "        #     return int(opt_result.num_iterations.numpy())\n",
        "        # add loss_batch to loss\n",
        "        opt_result = tfp.optimizer.lbfgs_minimize(func,\n",
        "            tf.convert_to_tensor(get_weights(), dtype=tf.float32),\n",
        "            max_iterations=epochs)\n",
        "    \n",
        "        print(opt_result)\n",
        "        # set_weights(result)\n",
        "        # print(result)\n",
        "\n",
        "        # start = get_weights()\n",
        "        # @tf.function\n",
        "        # def opt_with_bfgs():\n",
        "        #     return tfp.optimizer.lbfgs_minimize(\n",
        "        #         loss_grad,\n",
        "        #         initial_position=tf.convert_to_tensor(start, dtype=tf.float32),\n",
        "        #         max_iterations=epochs)\n",
        "\n",
        "        # results = run(opt_with_bfgs)\n",
        "        # print('BFGS Results')\n",
        "        # print('Converged:', results.converged)\n",
        "        # print('Location of the minimum:', results.position)\n",
        "        # print('Number of iterations:', results.num_iterations)\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E24sUnv2mlOK"
      },
      "source": [
        "train(200, optimizer='adam')\n",
        "train(400, optimizer='l-bfgs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z6WN9DFv1B2"
      },
      "source": [
        "def u(network,x,y,t):\n",
        "    z = tf.convert_to_tensor([[x, y, t]])\n",
        "    uvp = network(z)\n",
        "    u = float(uvp[:,0])\n",
        "    v = float(uvp[:,1])\n",
        "    return u, v\n",
        "\n",
        "def exact_sol(x,y,t):\n",
        "    Re = 100.0\n",
        "    u = - math.cos(pi*x)*math.sin(pi*y)*math.exp(-2.0*pi*pi*t/Re)\n",
        "    v = math.sin(pi*x)*math.cos(pi*y)*math.exp(-2.0*pi*pi*t/Re)\n",
        "    return u, v\n",
        "\n",
        "P = data_test\n",
        "\n",
        "err = 0\n",
        "for i in range(len(P)):\n",
        "    u_pred, v_pred = u(PDEmodel, P[i][0], P[i][1], P[i][2])\n",
        "    u_e, v_e = exact_sol(P[i][0], P[i][1], P[i][2])\n",
        "    err += (u_pred - u_e)**2 + (v_pred - v_e)**2\n",
        "    # if (i < 1000):\n",
        "    # print(u_e)\n",
        "    # print(u_pred)\n",
        "    # print(\"-----\")\n",
        "\n",
        "L2 = math.sqrt(err/len(P))\n",
        "print(\"L2 = \", L2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "losses = np.array(losses_hist)\n",
        "losses = losses.reshape(len(losses))\n",
        "epochs = losses.size\n",
        "x_epochs = [i+1 for i in range(epochs)]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(x_epochs,losses, color = 'blue')\n",
        "ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
        "plt.setp(ax, xlabel='Iteration')\n",
        "plt.setp(ax, ylabel='Loss')\n",
        "plt.show()\n",
        "\n",
        "loss_file = open(\"Loss.txt\", \"w\")\n",
        "for row in np.array(losses_hist).reshape(len(losses_hist), 1):\n",
        "    np.savetxt(loss_file, row)\n",
        "loss_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}